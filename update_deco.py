import json, requests, time, concurrent.futures, base58, re
from urllib.parse import urlparse

# å®šä¹‰ 2026 å¹´å·²çŸ¥çš„é«˜å¸¦å®½å¤§å‚æºï¼ˆè¿™äº›æºè™½ç„¶å»¶è¿Ÿé«˜ï¼Œä½†èƒ½æ‰›ä½ 4K æµé‡ï¼‰
# åªè¦è¿™äº›ç«™åœ¨ 10s å†…æœ‰å“åº”ï¼Œå°±å¼ºåˆ¶æ’åœ¨æœ€å‰é¢
PRIORITY_DOMAINS = [
    "lziapi.com", "ffzyapi.com", "huaceapi.com", "suoniapi.com", 
    "gture.top", "bfzyapi.com", "kkzy.tv", "feisuzyapi.com",
    "snzypm.com", "123zy.com", "zuidapi.com", "wolongzy.cc"
]

DYNAMIC_SOURCES = [
    "https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://itvbox.cc/tvbox/sources/my.json",
    "https://raw.liucn.cc/box/m.json"
]

def check_site_bandwidth_focus(site):
    """
    é«˜å®½å®¹åº¦æ£€æµ‹ï¼š
    1. å»¶è¿Ÿ 100ms å’Œ 1000ms å¯¹æˆ‘ä»¬æ¥è¯´æ²¡åŒºåˆ«ã€‚
    2. åªè¦èƒ½é€šï¼Œä¸”åœ¨ç™½åå•å†…ï¼Œå°±æ˜¯é¡¶çº§æºã€‚
    """
    try:
        start = time.time()
        # å°†è¶…æ—¶æ”¾å®½åˆ° 8 ç§’ï¼Œç¡®ä¿é‚£äº›â€œæ…¢çƒ­å‹â€çš„é«˜é€Ÿç«™ä¸è¢«å‰”é™¤
        res = requests.get(site['api'], timeout=8)
        if res.status_code == 200 and "vod" in res.text:
            domain = urlparse(site['api']).netloc
            # æƒé‡è®¡ç®—ï¼šç™½åå• 0 åˆ†ï¼Œæ™®é€šç«™ 100 åˆ†
            weight = 0 if any(k in domain for k in PRIORITY_DOMAINS) else 100
            return (weight, site)
    except:
        pass
    return None

def main():
    raw_pool = []
    # ç»“åˆåŠ¨æ€æŠ“å–å’Œä½ æä¾›çš„è“æœ¬æ•°æ®
    for url in DYNAMIC_SOURCES:
        try:
            r = requests.get(url, timeout=10)
            data = r.json()
            for s in data.get("sites", []):
                if s.get("type") in [0, 1] and "api.php" in s.get("api", ""):
                    name = re.sub(r'\(.*?\)|\[.*?\]|èµ„æº|é‡‡é›†|æé€Ÿ', '', s["name"]).strip()
                    raw_pool.append({"api": s["api"], "name": name or "æµ·å¤–é«˜é€Ÿæº"})
        except: continue

    # ä¸¥æ ¼åŸŸåå»é‡ï¼š50ä¸ªå‘ä½å¿…é¡»æ˜¯50ä¸ªä¸åŒçš„å‡ºå£
    unique_sites = {urlparse(s['api']).netloc: s for s in raw_pool}.values()

    # å¹¶å‘æ£€æµ‹
    with concurrent.futures.ThreadPoolExecutor(max_workers=60) as executor:
        results = [r for r in executor.map(check_site_bandwidth_focus, unique_sites) if r]
    
    # æ’åºé€»è¾‘ï¼šä¼˜å…ˆä¿ç™½åå•ï¼Œå‰©ä¸‹æŒ‰å‘ç°é¡ºåºè¡¥é½ï¼ˆä¸æŒ‰å»¶è¿Ÿæ’ï¼‰
    results.sort(key=lambda x: x[0])
    top_50 = [r[1] for r in results[:50]]

    # å…œåº•å¡«å……
    while len(top_50) < 50:
        top_50.append({"api": "https://cj.lziapi.com/api.php/provide/vod", "name": "é‡å­4Kä¿åº•"})

    # æ•´ç† DecoTV æ ¼å¼
    for i, s in enumerate(top_50):
        s['detail'] = s['api'].split("api.php")[0]
        # ç»™é«˜å¸¦å®½æºæ‰“ä¸Šé’»çŸ³æ ‡è®°
        if any(k in s['api'] for k in PRIORITY_DOMAINS):
            s['name'] = f"ğŸ’{s['name']}"

    config = {
        "cache_time": 9200,
        "api_site": {f"api_{i+1}": s for i, s in enumerate(top_50)},
        "custom_category": [
            {"name": "ğŸï¸ 115Â·ç½‘ç›˜é«˜æ¸…", "type": "movie", "query": "115"},
            {"name": "ğŸ”¥ 4KÂ·ææ¸…ä¸“åŒº", "type": "movie", "query": "4K"},
            {"name": "ğŸ“º åè¯­ç²¾é€‰", "type": "movie", "query": "åè¯­"}
        ]
    }

    # å†™å…¥æ–‡ä»¶
    with open("deco.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

    compact = json.dumps(config, ensure_ascii=False).encode('utf-8')
    with open("deco_b58.txt", "w", encoding="utf-8") as f:
        f.write(base58.b58encode(compact).decode('utf-8'))

if __name__ == "__main__":
    main()
