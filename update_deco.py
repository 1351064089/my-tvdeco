import json, requests, time, concurrent.futures, base58, re
from urllib.parse import urlparse

# 2026å¹´ç²¾é€‰æºï¼Œæ¶µç›–å…¨ç½‘ 90% ä»¥ä¸Šçš„é‡‡é›†ç«™
SOURCE_URLS = [
    "https://gh-proxy.com/https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://raw.liucn.cc/box/m.json",
    "https://itvbox.cc/tvbox/sources/my.json",
    "https://raw.githubusercontent.com/FongMi/Release/main/levon/Index.json"
]

BACKUP_SITES = [
    {"api": "https://cj.lziapi.com/api.php/provide/vod", "name": "é‡å­é«˜é€Ÿ", "detail": "https://cj.lziapi.com"},
    {"api": "https://cj.ffzyapi.com/api.php/provide/vod", "name": "éå‡¡æé€Ÿ", "detail": "https://cj.ffzyapi.com"},
    {"api": "https://cj.huaceapi.com/api.php/provide/vod", "name": "åç­–4K", "detail": "https://cj.huaceapi.com"},
    {"api": "https://video.gture.top/api.php/provide/vod", "name": "å…‰é€Ÿè“å…‰", "detail": "https://video.gture.top"},
    {"api": "https://bfzyapi.com/api.php/provide/vod", "name": "æš´é£å½±éŸ³", "detail": "https://bfzyapi.com"}
]

def check_site(site):
    try:
        start = time.time()
        # åªè¦æµ‹é€Ÿé€šäº†ï¼Œå¹¶ä¸”åŒ…å« vod å­—æ ·è¯´æ˜æ¥å£æ­£å¸¸
        res = requests.get(site['api'], timeout=2.5)
        if res.status_code == 200 and "vod" in res.text:
            delay = time.time() - start
            return (delay, site)
    except: pass
    return None

def main():
    pool = []
    for url in SOURCE_URLS:
        try:
            r = requests.get(url, timeout=10)
            data = json.loads(r.text.encode('utf-8').decode('utf-8-sig'))
            for s in data.get("sites", []):
                if s.get("type") in [0, 1] and "api.php" in s.get("api", ""):
                    name = re.sub(r'\(.*?\)|\[.*?\]|èµ„æº|é‡‡é›†|æé€Ÿ|ä¼˜è´¨|å®˜ç½‘', '', s["name"]).strip()
                    pool.append({"api": s["api"], "name": name if name else "æé€Ÿæº"})
        except: continue

    # --- æ ¸å¿ƒå»é‡é€»è¾‘ ---
    # ä½¿ç”¨åŸŸåä½œä¸º Keyï¼Œç¡®ä¿åŒä¸€ä¸ªæœåŠ¡å™¨åªå‡ºç°ä¸€æ¬¡
    domain_unique_pool = {}
    for s in pool:
        domain = urlparse(s['api']).netloc
        # å¦‚æœåŸŸåé‡å¤ï¼Œåªä¿ç•™åå­—é•¿çš„ï¼ˆé€šå¸¸åå­—æ›´å®Œæ•´ï¼‰
        if domain not in domain_unique_pool or len(s['name']) > len(domain_unique_pool[domain]['name']):
            domain_unique_pool[domain] = s
    
    # --- å¤šçº¿ç¨‹å¹¶å‘æµ‹é€Ÿ ---
    unique_list = list(domain_unique_pool.values())
    with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
        results = sorted([r for r in executor.map(check_site, unique_list) if r], key=lambda x: x[0])
    
    top_list = [r[1] for r in results[:50]]

    # --- å¼ºåˆ¶è¡¥é½ä¸ç»†èŠ‚å¤„ç† ---
    while len(top_list) < 50:
        top_list.append(BACKUP_SITES[len(top_list) % len(BACKUP_SITES)])

    # è§„èŒƒåŒ–è¾“å‡ºå†…å®¹
    for i, s in enumerate(top_list):
        s['detail'] = s['api'].split("api.php")[0]
        # ç»™å‰ 5 ä¸ªæœ€å¿«çš„ç«™åŠ ä¸ªç«è‹—å›¾æ ‡
        if i < 5: s['name'] = f"ğŸ”¥{s['name']}"

    config = {
        "cache_time": 7200,
        "api_site": {f"site_{i:02d}": s for i, s in enumerate(top_list)},
        "custom_category": [
            {"name": "ğŸ”¥ 4KÂ·ææ¸…", "type": "movie", "query": "4K"},
            {"name": "ğŸï¸ åè¯­å¤§ç‰‡", "type": "movie", "query": "åè¯­"},
            {"name": "ğŸŒ¸ 2026æ–°ç•ª", "type": "anime", "query": "2026"}
        ]
    }

    with open("deco.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    compact = json.dumps(config, ensure_ascii=False).encode('utf-8')
    with open("deco_b58.txt", "w", encoding="utf-8") as f:
        f.write(base58.b58encode(compact).decode('utf-8'))

if __name__ == "__main__":
    main()
