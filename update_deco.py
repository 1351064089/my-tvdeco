import json, requests, time, concurrent.futures, base58, re
from urllib.parse import urlparse

# 1. åŠ¨æ€æŠ“å–æºï¼ˆ2026å¹´æœ€æ´»è·ƒçš„èšåˆè®¢é˜…ï¼‰
DYNAMIC_SOURCES = [
    "https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://itvbox.cc/tvbox/sources/my.json",
    "https://raw.liucn.cc/box/m.json"
]

# 2. ä½ çš„æ ¸å¿ƒä¿åº•åº“ï¼ˆç²¾é€‰è‡ªä½ æä¾›çš„åˆ—è¡¨ï¼‰
CORE_SITES = [
    {"api": "https://cj.lziapi.com/api.php/provide/vod", "name": "é‡å­èµ„æº"},
    {"api": "https://api.ffzyapi.com/api.php/provide/vod", "name": "éå‡¡å½±è§†"},
    {"api": "https://jszyapi.com/api.php/provide/vod", "name": "æé€Ÿèµ„æº"},
    {"api": "https://api.guangsuapi.com/api.php/provide/vod", "name": "å…‰é€Ÿèµ„æº"},
    {"api": "https://suoniapi.com/api.php/provide/vod", "name": "ç´¢å°¼èµ„æº"},
    {"api": "https://bfzyapi.com/api.php/provide/vod", "name": "æš´é£é«˜æ¸…"},
    {"api": "https://hhzyapi.com/api.php/provide/vod", "name": "è±ªåèµ„æº"},
    {"api": "https://api.1080zyku.com/inc/api_mac10.php", "name": "1080èµ„æº"}
]

def check_site(site):
    """æµ‹é€Ÿå¹¶éªŒè¯æ¥å£æœ‰æ•ˆæ€§"""
    try:
        start = time.time()
        # å¢åŠ è¶…æ—¶é™åˆ¶ï¼Œå¤ªæ…¢çš„ç›´æ¥ä¸è¦
        res = requests.get(site['api'], timeout=2)
        if res.status_code == 200 and ("vod" in res.text or "list" in res.text):
            return (time.time() - start, site)
    except:
        pass
    return None

def main():
    all_raw_sites = CORE_SITES.copy()

    # è‡ªåŠ¨æŠ“å–å…¨ç½‘æœ€æ–°åœ°å€
    for url in DYNAMIC_SOURCES:
        try:
            r = requests.get(url, timeout=5)
            data = r.json()
            for s in data.get("sites", []):
                if s.get("type") in [0, 1] and "api.php" in s.get("api", ""):
                    name = re.sub(r'\(.*?\)|\[.*?\]|èµ„æº|é‡‡é›†', '', s["name"]).strip()
                    all_raw_sites.append({"api": s["api"], "name": name or "è‡ªåŠ¨å‘ç°"})
        except:
            continue

    # åŸŸåå»é‡ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼šé˜²æ­¢é‡å¤ç«™å æ®50ä¸ªåé¢ï¼‰
    unique_dict = {}
    for s in all_raw_sites:
        domain = urlparse(s['api']).netloc
        if domain and domain not in unique_dict:
            unique_dict[domain] = s

    # å¹¶å‘æµ‹é€Ÿç­›é€‰ï¼ˆå‰50åï¼‰
    with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
        results = [r for r in executor.map(check_site, unique_dict.values()) if r]
    
    # æŒ‰é€Ÿåº¦æ’åºï¼Œå–å‰50ä¸ª
    results.sort(key=lambda x: x[0])
    top_50 = [r[1] for r in results[:50]]

    # è¡¥é½è‡³50ä¸ªï¼ˆå¦‚æœæŠ“åˆ°çš„ä¸å¤Ÿï¼Œå°±å¾ªç¯è¡¥é½ï¼‰
    while len(top_50) < 50:
        top_50.append(CORE_SITES[len(top_50) % len(CORE_SITES)])

    # æ•´ç†æ ¼å¼
    for i, s in enumerate(top_50):
        s['detail'] = s['api'].split("api.php")[0]
        if i < 5: s['name'] = f"ğŸš€{s['name']}" # ç»™æœ€å¿«çš„5ä¸ªåŠ æ ‡è®°

    config = {
        "cache_time": 9200,
        "api_site": {f"api_{i+1}": s for i, s in enumerate(top_50)},
        "custom_category": [
            {"name": "ğŸï¸ 115Â·è“å…‰", "type": "movie", "query": "115"},
            {"name": "ğŸ”¥ 4KÂ·ææ¸…", "type": "movie", "query": "4K"},
            {"name": "ğŸŒ¸ 2026æ–°ç•ª", "type": "anime", "query": "2026"}
        ]
    }

    # å¯¼å‡ºæ–‡ä»¶
    with open("deco.json", "w", encoding="utf-8") as f
