import json, requests, time, base58, re
from urllib.parse import urlparse

# 2026å¹´å›½å†…æœ€ç¨³ã€å¸¦å®½æœ€é«˜çš„é¡¶çº§å¤§å‚æºç™½åå•
# è¿™äº›ç«™ç”±å›½å†…é«˜é€ŸCDNåˆ†å‘ï¼Œå¤§å±ç«¯æ’­æ”¾æé€Ÿ
PREMIUM_SITES = [
    {"api": "https://cj.lziapi.com/api.php/provide/vod", "name": "ğŸ”¥é‡å­é«˜æ¸…4K"},
    {"api": "https://cj.ffzyapi.com/api.php/provide/vod", "name": "ğŸ”¥éå‡¡ç§’å¼€"},
    {"api": "https://cj.huaceapi.com/api.php/provide/vod", "name": "åç­–ææ¸…"},
    {"api": "https://video.gture.top/api.php/provide/vod", "name": "å…‰é€Ÿè“å…‰"},
    {"api": "https://bfzyapi.com/api.php/provide/vod", "name": "æš´é£å½±è§†"},
    {"api": "https://www.605zy.cc/api.php/provide/vod", "name": "605èµ„æº"},
    {"api": "https://api.tianyiapi.com/api.php/provide/vod", "name": "å¤©ç¿¼é«˜æ¸…"},
    {"api": "https://jszyapi.com/api.php/provide/vod", "name": "æé€Ÿèµ„æº"},
    {"api": "https://www.feisuzyapi.com/api.php/provide/vod", "name": "é£é€Ÿèµ„æº"},
    {"api": "https://api.kkzy.tv/api.php/provide/vod", "name": "å¿«çœ‹èµ„æº"},
    {"api": "https://subocaiji.com/api.php/provide/vod", "name": "é€Ÿæ’­èµ„æº"},
    {"api": "https://cj.sdzyapi.com/api.php/provide/vod", "name": "é—ªç”µèµ„æº"},
    {"api": "https://www.kuaichezy.com/api.php/provide/vod", "name": "å¿«è½¦èµ„æº"},
    {"api": "https://api.123zy.com/api.php/provide/vod", "name": "123èµ„æº"},
    {"api": "https://www.jingchengzy.com/api.php/provide/vod", "name": "ç²¾å“èµ„æº"}
]

# å¤‡ç”¨çˆ¬å–æ± 
SOURCE_URLS = [
    "https://gh-proxy.com/https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://raw.liucn.cc/box/m.json",
    "https://itvbox.cc/tvbox/sources/my.json"
]

def main():
    final_50 = []
    seen_domains = set()

    # 1. é¦–å…ˆå¡«å…¥ç¡¬æ ¸ä¼˜é€‰æº
    for s in PREMIUM_SITES:
        domain = urlparse(s['api']).netloc
        if domain not in seen_domains:
            s['detail'] = s['api'].split("api.php")[0]
            final_50.append(s)
            seen_domains.add(domain)

    # 2. ä»å…¨ç½‘æºä¸­è¡¥é½å‰©ä¸‹çš„ä½ç½®
    for url in SOURCE_URLS:
        if len(final_50) >= 50: break
        try:
            r = requests.get(url, timeout=10)
            data = json.loads(r.text.encode('utf-8').decode('utf-8-sig'))
            for s in data.get("sites", []):
                if len(final_50) >= 50: break
                api = s.get("api", "")
                if s.get("type") in [0, 1] and "api.php" in api:
                    domain = urlparse(api).netloc
                    if domain not in seen_domains:
                        name = re.sub(r'\(.*?\)|\[.*?\]|èµ„æº|é‡‡é›†|æé€Ÿ|ä¼˜è´¨', '', s["name"]).strip()
                        final_50.append({
                            "api": api,
                            "name": name if name else "å¤‡ç”¨çº¿è·¯",
                            "detail": api.split("api.php")[0]
                        })
                        seen_domains.add(domain)
        except: continue

    # ç¡®ä¿æ­£å¥½ 50 ä¸ªï¼Œä¸å¤šä¸å°‘
    final_50 = final_50[:50]

    config = {
        "cache_time": 7200,
        "api_site": {f"site_{i:02d}": s for i, s in enumerate(final_50)},
        "custom_category": [
            {"name": "ğŸ”¥ 4KÂ·ææ¸…", "type": "movie", "query": "4K"},
            {"name": "ğŸï¸ åè¯­ç²¾é€‰", "type": "movie", "query": "åè¯­"},
            {"name": "ğŸŒ¸ 2026æ–°ç•ª", "type": "anime", "query": "2026"}
        ]
    }

    # ä¿å­˜ JSON
    with open("deco.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆ Base58
    compact = json.dumps(config, ensure_ascii=False).encode('utf-8')
    with open("deco_b58.txt", "w", encoding="utf-8") as f:
        f.write(base58.b58encode(compact).decode('utf-8'))

if __name__ == "__main__":
    main()
