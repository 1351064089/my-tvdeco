import json, requests, time, concurrent.futures, base58, re

# ç²¾é€‰ 2026 å¹´æœ€ç¨³çš„å…ƒæ•°æ®æ± ï¼Œç¡®ä¿èƒ½æŠ“åˆ°çœŸå®ç«™ç‚¹
SOURCE_URLS = [
    "https://gh-proxy.com/https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://raw.liucn.cc/box/m.json",
    "https://itvbox.cc/tvbox/sources/my.json"
]

# æ ¸å¿ƒé«˜é€Ÿç™½åå•ï¼šè¿™äº›æºå¿…é¡»æ’åœ¨å‰é¢
VIP_KEYWORDS = ['lzi', 'ffzy', 'huace', 'bfzy', 'snzy', 'kuaikan']

def evaluate_site(site):
    """æ·±åº¦éªŒè¯ï¼šä¸ä»…æµ‹å»¶è¿Ÿï¼Œè¿˜éªŒè¯æ¥å£æ˜¯å¦çœŸçš„èƒ½åå‡ºæ•°æ®"""
    api_url = site['api']
    if "api.php" not in api_url: return None
    
    try:
        start_time = time.time()
        # å°è¯•è¯·æ±‚æœ€æ–°çš„1æ¡æ•°æ®æ¥éªŒè¯æ¥å£çœŸå®æœ‰æ•ˆæ€§
        test_url = f"{api_url}?ac=list&pagesize=1"
        res = requests.get(test_url, timeout=2.5, headers={'User-Agent': 'DecoTV/2.1'})
        
        if res.status_code == 200 and ("vod" in res.text or "list" in res.text):
            delay = time.time() - start_time
            # æƒé‡è®¡ç®—ï¼šå¦‚æœæ˜¯å¤§å‚é«˜é€ŸCDNï¼Œç»™äºˆ 0.5ç§’ çš„â€œåŠ é€Ÿç‰¹æƒâ€æ’å
            score = delay
            if any(k in api_url.lower() for k in VIP_KEYWORDS):
                score -= 0.5
            return (score, site)
    except:
        pass
    return None

def main():
    raw_pool = []
    for url in SOURCE_URLS:
        try:
            res = requests.get(url, timeout=10)
            data = json.loads(res.text.encode('utf-8').decode('utf-8-sig'))
            for s in data.get("sites", []):
                if s.get("type") in [0, 1] and s.get("api").startswith("http"):
                    # ç«™åç²¾ç®€åŒ–å¤„ç†
                    name = re.sub(r'\(.*?\)|\[.*?\]|èµ„æº|é‡‡é›†|æé€Ÿ|ä¼˜è´¨', '', s["name"]).strip()
                    raw_pool.append({
                        "api": s["api"],
                        "name": name if name else "é«˜é€Ÿæ¥å£",
                        "detail": s["api"].split("api.php")[0]
                    })
        except: continue

    # ç‰©ç†å»é‡
    unique_sites = {s['api']: s for s in raw_pool}.values()

    # å¹¶å‘éªŒè¯ï¼ˆæé«˜åˆ° 60 çº¿ç¨‹åŠ é€Ÿå¤„ç†ï¼‰
    with concurrent.futures.ThreadPoolExecutor(max_workers=60) as executor:
        results = [r for r in executor.map(evaluate_site, unique_sites) if r]
    
    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
    results.sort(key=lambda x: x[0])
    
    # æå–å‰ 50 ä¸ªæœ€å¼ºç«™ç‚¹
    final_50 = [r[1] for r in results[:50]]

    # å…œåº•ï¼šå¦‚æœæŠ“å–çš„ä¸è¶³50ï¼Œç”¨ä¸åŒçš„å¤§å‚æºå¾ªç¯å¡«å……ï¼Œä¿è¯åå­—çœŸå®ä¸”ä¸é‡å¤
    if len(final_50) < 50:
        backups = [
            {"api": "https://cj.lziapi.com/api.php/provide/vod", "name": "é‡å­é«˜æ¸…", "detail": "https://cj.lziapi.com"},
            {"api": "https://cj.ffzyapi.com/api.php/provide/vod", "name": "éå‡¡ç§’å¼€", "detail": "https://cj.ffzyapi.com"},
            {"api": "https://cj.huaceapi.com/api.php/provide/vod", "name": "åç­–4K", "detail": "https://cj.huaceapi.com"}
        ]
        while len(final_50) < 50:
            final_50.append(backups[len(final_50) % len(backups)])

    config = {
        "cache_time": 7200,
        "api_site": {f"site_{i:02d}": s for i, s in enumerate(final_50)},
        "custom_category": [
            {"name": "ğŸ”¥ 4KÂ·ææ¸…", "type": "movie", "query": "4K"},
            {"name": "ğŸï¸ åè¯­ç²¾é€‰", "type": "movie", "query": "åè¯­"},
            {"name": "ğŸŒ¸ 2026æ–°ç•ª", "type": "anime", "query": "2026"}
        ]
    }

    # è¾“å‡ºæ–‡ä»¶
    with open("deco.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆ Base58
    compact_json = json.dumps(config, ensure_ascii=False).encode('utf-8')
    with open("deco_b58.txt", "w", encoding="utf-8") as f:
        f.write(base58.b58encode(compact_json).decode('utf-8'))

if __name__ == "__main__":
    main()
