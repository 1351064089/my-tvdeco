import json, requests, time, concurrent.futures, base58, re

# æ›´æ¢ä¸º 2026 å¹´æœ€æ´»è·ƒçš„ä¸‰ä¸ªç¨³å®šèšåˆæº
SOURCE_URLS = [
    "https://gh-proxy.com/https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://gh-proxy.com/https://raw.githubusercontent.com/FongMi/Release/main/levon/Index.json",
    "https://raw.liucn.cc/box/m.json" 
]

# å¤‡ç”¨çœŸå®æºï¼ˆå¦‚æœæŠ“å–ä¸è¶³ï¼Œç”¨è¿™äº›ç»å¯¹æœ‰æ•ˆçš„ç«™è¡¥é½ï¼‰
BACKUP_SITES = [
    {"api": "https://cj.lziapi.com/api.php/provide/vod", "name": "é‡å­èµ„æº", "detail": "https://cj.lziapi.com"},
    {"api": "https://cj.ffzyapi.com/api.php/provide/vod", "name": "éå‡¡èµ„æº", "detail": "https://cj.ffzyapi.com"},
    {"api": "https://cj.huaceapi.com/api.php/provide/vod", "name": "åç­–ææ¸…", "detail": "https://cj.huaceapi.com"},
    {"api": "https://bfzyapi.com/api.php/provide/vod", "name": "æš´é£é«˜æ¸…", "detail": "https://bfzyapi.com"}
]

def evaluate_site(site):
    try:
        # åªå– CMS ç±»å‹çš„ api
        if "api.php" not in site['api']: return None
        start = time.time()
        # å¢åŠ  headers æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼Œé˜²æ­¢è¢«æ‹¦æˆª
        headers = {'User-Agent': 'Mozilla/5.0 DecoTV/2026'}
        res = requests.get(site['api'], timeout=2, headers=headers)
        if res.status_code == 200 and ("vod" in res.text or "code" in res.text):
            delay = time.time() - start
            return (delay, site)
    except: pass
    return None

def main():
    raw_sites = []
    for url in SOURCE_URLS:
        try:
            print(f"æ­£åœ¨æ‹‰å–æº: {url}")
            res = requests.get(url, timeout=10)
            # å…¼å®¹å¤„ç†ï¼šæœ‰äº›æºå¯èƒ½å¸¦ BOM å¤´æˆ–æ ¼å¼ä¸è§„èŒƒ
            content = res.text.encode('utf-8').decode('utf-8-sig')
            data = json.loads(content)
            
            for s in data.get("sites", []):
                # æå–ç«™åå¹¶æ¸…æ´—
                name = re.sub(r'\(.*?\)|\[.*?\]|èµ„æº|é‡‡é›†|å®˜ç½‘|ç»¼åˆ', '', s.get("name", ""))
                api = s.get("api", "")
                if api.startswith("http"):
                    raw_sites.append({
                        "api": api,
                        "name": name.strip() or "ä¼˜è´¨çº¿è·¯",
                        "detail": api.split("api.php")[0]
                    })
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥ {url}: {e}")
            continue

    # 1. ç‰©ç†å»é‡
    unique_dict = {s['api']: s for s in raw_sites}
    
    # 2. å¤šçº¿ç¨‹æµ‹é€Ÿæ’åº
    with concurrent.futures.ThreadPoolExecutor(max_workers=50) as exe:
        valid_results = [r for r in exe.map(evaluate_site, unique_dict.values()) if r]
        valid_results.sort(key=lambda x: x[0]) # å»¶è¿Ÿä½æ’å‰é¢
    
    final_list = [r[1] for r in valid_results[:50]]

    # 3. å¼ºåˆ¶å‡‘æ»¡ 50 ä¸ªï¼Œç¡®ä¿ä¸æ˜¾ç¤ºâ€œä¿åº•çº¿è·¯â€è€Œæ˜¯çœŸå®çš„å¤‡ä»½åç§°
    while len(final_list) < 50:
        final_list.append(BACKUP_SITES[len(final_list) % len(BACKUP_SITES)])

    # 4. ç”Ÿæˆé…ç½®
    config = {
        "cache_time": 7200,
        "api_site": {f"site_{i:02d}": s for i, s in enumerate(final_list)},
        "custom_category": [
            {"name": "ğŸ”¥ 4KÂ·ææ¸…", "type": "movie", "query": "4K"},
            {"name": "ğŸï¸ åè¯­å¤§ç‰‡", "type": "movie", "query": "åè¯­"},
            {"name": "ğŸŒ¸ 2026æ–°ç•ª", "type": "anime", "query": "2026"}
        ]
    }
    
    # ä¿å­˜ JSON
    with open("deco.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
        
    # ä¿å­˜ Base58
    compact_json = json.dumps(config, ensure_ascii=False).encode('utf-8')
    with open("deco_b58.txt", "w", encoding="utf-8") as f:
        f.write(base58.b58encode(compact_json).decode('utf-8'))
    print("æ›´æ–°æˆåŠŸï¼Œå·²ç”Ÿæˆ 50 ä¸ªçœŸå®ç«™ç‚¹ã€‚")

if __name__ == "__main__":
    main()
