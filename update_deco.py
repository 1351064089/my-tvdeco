import json, requests, time, concurrent.futures

# æ‰©å……æ›´å…¨çš„é«˜è´¨é‡èšåˆæºï¼Œç¡®ä¿æŠ“å–åŸºæ•°è¶³å¤Ÿå¤§
SOURCE_URLS = [
    "https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://raw.githubusercontent.com/FongMi/Release/main/levon/Index.json",
    "https://raw.githubusercontent.com/yqmkk/my-tv-config/main/tv.json",
    "https://raw.githubusercontent.com/1351064089/my-tv-config/main/tv.json",
    "https://itvbox.cc/tvbox/sources/my.json"
]

# å®šä¹‰çœŸå®å¤‡ç”¨æºï¼ˆå½“æŠ“å–ä¸è¶³50ä¸ªæ—¶ï¼Œå¾ªç¯ä½¿ç”¨è¿™äº›çœŸå®ç«™åå¡«å……ï¼‰
BACKUP_SITES = [
    {"api": "https://cj.lziapi.com/api.php/provide/vod", "name": "é‡å­èµ„æº", "detail": "https://cj.lziapi.com"},
    {"api": "https://cj.ffzyapi.com/api.php/provide/vod", "name": "éå‡¡èµ„æº", "detail": "https://cj.ffzyapi.com"},
    {"api": "https://video.gture.top/api.php/provide/vod", "name": "å…‰é€Ÿèµ„æº", "detail": "https://video.gture.top"},
    {"api": "https://bfzyapi.com/api.php/provide/vod", "name": "æš´é£èµ„æº", "detail": "https://bfzyapi.com"},
    {"api": "https://cj.huaceapi.com/api.php/provide/vod", "name": "åç­–ææ¸…", "detail": "https://cj.huaceapi.com"}
]

def evaluate_site(site):
    try:
        start_time = time.time()
        # ä¸¥æ ¼ 2 ç§’è¶…æ—¶ï¼Œç¡®ä¿åªæœ‰æé€Ÿç«™å…¥é€‰
        res = requests.get(site['api'], timeout=2)
        if res.status_code == 200 and "vod" in res.text:
            delay = time.time() - start_time
            # ç»™å¤§å‚ç«™ï¼ˆCDNå¼ºï¼‰åŠ æƒï¼Œè®©å®ƒä»¬æ’åœ¨å‰é¢
            score = delay
            if any(fast in site['api'] for fast in ['lzi', 'ffzy', 'huace', 'bfzy']):
                score -= 0.3
            return (score, site)
    except:
        pass
    return None

def main():
    all_raw_sites = []
    for url in SOURCE_URLS:
        try:
            data = requests.get(url, timeout=8).json()
            for s in data.get("sites", []):
                # åªé‡‡é›† CMS æ¥å£
                if s.get("type") in [0, 1] and "api.php" in s.get("api", ""):
                    # æ¸…ç†ç«™åä¸­çš„æ‚è´¨
                    clean_name = s["name"].replace("èµ„æº", "").replace("é‡‡é›†", "").strip()
                    all_raw_sites.append({
                        "api": s["api"],
                        "name": clean_name if clean_name else "æœªçŸ¥ç«™ç‚¹",
                        "detail": s["api"].split("api.php")[0]
                    })
        except: continue

    # å»é‡
    unique_sites = {s['api']: s for s in all_raw_sites}.values()

    # å¹¶å‘æµ‹é€Ÿ
    with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
        results = [r for r in executor.map(evaluate_site, unique_sites) if r]
        
    results.sort(key=lambda x: x[0])
    top_50 = [r[1] for r in results[:50]]

    # å¡«å……é€»è¾‘ä¼˜åŒ–ï¼šå¦‚æœä¸è¶³50ä¸ªï¼Œä»çœŸå®å¤‡ä»½æºä¸­å¾ªç¯æå–
    if len(top_50) < 50:
        backup_idx = 0
        while len(top_50) < 50:
            top_50.append(BACKUP_SITES[backup_idx % len(BACKUP_SITES)])
            backup_idx += 1

    config = {
        "cache_time": 7200,
        "api_site": {f"site_{i:02d}": s for i, s in enumerate(top_50)},
        "custom_category": [
            {"name": "ğŸ”¥ 4KÂ·ææ¸…", "type": "movie", "query": "4K"},
            {"name": "ğŸï¸ åè¯­å¤§ç‰‡", "type": "movie", "query": "åè¯­"},
            {"name": "ğŸŒ¸ 2026æ–°ç•ª", "type": "anime", "query": "2026"},
            {"name": "ğŸ¥ æ¬§ç¾è“å…‰", "type": "movie", "query": "è“å…‰"}
        ]
    }
    
    with open("deco.json", "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()
